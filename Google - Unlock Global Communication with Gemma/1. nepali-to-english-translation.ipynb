{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":85986,"sourceType":"modelInstanceVersion","modelInstanceId":72246,"modelId":78150}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma - Translator of Old Korean Literature\n\nThe Korean alphabet, or Hangul, has undergone changes over time, resulting in several letters no longer used in modern Korean. These obsolete letters include:\n\n1. ㆍ (Arae-a): This dot vowel represents a short 'a' sound.\n2. ㆆ (Yeorin-hieut): Pronounced as a 'light h,' akin to a softer version of the English 'h.'\n3. ㅿ (Bansiot): Represents the 'z' sound.\n4. ㆁ (Yet-ieung): A velar nasal sound comparable to 'ng' in the word 'sing.'\n\nFor Korean speakers, reading older literature presents a challenge due to the utilization of now-obsolete letters. Early Hangul lacked spaces between words, further complicating readability. In contrast, modern Hangul employs spaces, consistent with most alphabetic systems.\n\nHowever, with the capabilities provided by Gemma, it becomes possible to create a translator that can aid in understanding and bridging the gap between contemporary and archaic Korean.","metadata":{}},{"cell_type":"markdown","source":"## Using accelerators\n\nWe will focus on using the **free GPU from Kaggle** here.","metadata":{}},{"cell_type":"markdown","source":"## Before you begin\n\n### Gemma setup\n\nTo complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n\nGemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n\n- Sign in or register at [kaggle.com](https://www.kaggle.com)\n- Open the [Gemma 2 model card](https://www.kaggle.com/models/google/gemma-2) and select _\"Request Access\"_\n- Complete the consent form and accept the terms and conditions","metadata":{}},{"cell_type":"markdown","source":"## Install dependencies\n\nInstall Keras and KerasNLP.","metadata":{}},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp datasets\n!pip install -q -U keras\n\nimport os\n\n# Set the backbend before importing Keras\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n\nimport keras_nlp\nimport keras\n\n# Run at half precision.\n#keras.config.set_floatx(\"bfloat16\")\n\n# Training Configurations\ntoken_limit = 256\nlora_name = \"translator\"\nlora_rank = 4\nlr_value = 1e-4\ntrain_epoch = 5\nmodel_id = \"gemma2_instruct_2b_en\"","metadata":{"execution":{"iopub.status.busy":"2024-12-01T17:00:06.275863Z","iopub.execute_input":"2024-12-01T17:00:06.276444Z","iopub.status.idle":"2024-12-01T17:00:41.134118Z","shell.execute_reply.started":"2024-12-01T17:00:06.276411Z","shell.execute_reply":"2024-12-01T17:00:41.133068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Dataset\n\nHere's [the dataset](https://huggingface.co/datasets/bebechien/HongGildongJeon) from Hong Gildong jeon (Korean: 홍길동전), which is a Korean novel written during the Joseon Dynasty. The [original source](https://ko.wikisource.org/wiki/%ED%99%8D%EA%B8%B8%EB%8F%99%EC%A0%84_36%EC%9E%A5_%EC%99%84%ED%8C%90%EB%B3%B8) is in public domain. You will use a [modern translation](https://ko.wikisource.org/wiki/%ED%99%8D%EA%B8%B8%EB%8F%99%EC%A0%84_36%EC%9E%A5_%EC%99%84%ED%8C%90%EB%B3%B8/%ED%98%84%EB%8C%80%EC%96%B4_%ED%95%B4%EC%84%9D) in a [creative commons license](https://creativecommons.org/licenses/by-sa/4.0/), translated by `직지프로`.\n\nTo simplify the task, you will adopt the following structure for fine-tuning the model. The model will generate contemporary Korean text based on the user's input in Early Hangul.\n\n```\n<start_of_turn>user\\n\n됴션국셰둉ᄃᆡ왕즉위십오연의홍희문밧긔ᄒᆞᆫᄌᆡ상이잇스되\n<end_of_turn>\\n\n<start_of_turn>model\\n\n조선국 세종대왕 즉위 십오년에 홍회문 밖에 한 재상이 있으되,\n```\n\n> NOTE: korean text means, In the fifteenth year of the reign of King Sejong of Joseon, there was a prime minister outside Honghoemun Gate.\n","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ntokenizer = keras_nlp.models.GemmaTokenizer.from_preset(model_id)\n\nds_nep = load_dataset(\"ashokpoudel/nepali-english-translation-dataset\")\nprint(ds_nep)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:11:11.463514Z","iopub.execute_input":"2024-12-01T17:11:11.463872Z","iopub.status.idle":"2024-12-01T17:11:14.333271Z","shell.execute_reply.started":"2024-12-01T17:11:11.463843Z","shell.execute_reply":"2024-12-01T17:11:14.332330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for d in ds_nep['train']:\n    print(d)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:01:42.430627Z","iopub.execute_input":"2024-12-01T17:01:42.431107Z","iopub.status.idle":"2024-12-01T17:01:42.438007Z","shell.execute_reply.started":"2024-12-01T17:01:42.431080Z","shell.execute_reply":"2024-12-01T17:01:42.437180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data = ds_nep.with_format(\n#     \"train\", columns=[\"en\", \"np\"], output_all_columns=False\n# )\nprint(f'data: {ds_nep}')\nprint(f\"\\n\\n data['train']: {ds_nep['train']}\")\n\ntrain = []\n\n\n# Test with only 10k rows\ncount = 0\nmax_count = 1_000\nfor x in ds_nep['train']:\n    count+=1\n    if count > max_count: break\n    item = f\"<start_of_turn>user\\n{x['np']}<end_of_turn>\\n<start_of_turn>model\\n{x['en']}<end_of_turn>\"\n    length = len(tokenizer(item))\n    # skip data if the token length is longer than our limit\n    if length < token_limit:\n        train.append(item)\n\nprint(len(train))\nprint(train[0])\nprint(train[1])\nprint(train[2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:11:16.274834Z","iopub.execute_input":"2024-12-01T17:11:16.275209Z","iopub.status.idle":"2024-12-01T17:11:20.849260Z","shell.execute_reply.started":"2024-12-01T17:11:16.275179Z","shell.execute_reply":"2024-12-01T17:11:20.848304Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Model","metadata":{}},{"cell_type":"code","source":"import keras\nimport keras_nlp\n\nimport time\n\ngemma = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma.summary()\n\ntick_start = 0\n\n\ndef tick():\n    global tick_start\n    tick_start = time.time()\n\n\ndef tock():\n    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n\n\ndef text_gen(prompt):\n    tick()\n    input = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n    output = gemma.generate(input, max_length=token_limit)\n    print(\"\\nGemma output:\")\n    print(output)\n    tock()\n\ntext_gen(\"रामले भात खायो। \")\ntext_gen(\"कर्तब्य के छ जन को येति छन् बिरोध ? नक्श्यत्र हेर नभ मा दिल साथ सोध \")\ntext_gen(\"विश्व का सर्वोच्च शिखर सगरमाथा हॊ। \")\n    \n# text_gen(\"ᄃᆡ작ᄒᆞ여그ᄭᅩᆺ치흣터지거ᄂᆞᆯ\")\n# text_gen(\n#     \"금두겁이품의드러뵈니일졍ᄌᆡᄌᆞᄅᆞᆯ나흐리로다ᄒᆞ더니과연그ᄃᆞᆯ부터잉ᄐᆡᄒᆞ여십삭이차니\"\n# )\n# text_gen(\n#     \"이ᄯᆡᄂᆞᆫᄉᆞ월초팔일이라이날밤의오ᄉᆡᆨ구룸이집을두루고향ᄂᆡ진동ᄒᆞ며션녀ᄒᆞᆫᄡᅣᆼ이촉을들고드러와김ᄉᆡᆼᄃᆞ려니르ᄃᆡ\"\n# )\n# text_gen(\"ᄌᆡ히길너텬졍을어긔지말으소셔이아희ᄇᆡ필은낙양니샹셔집아ᄌᆡ니\")","metadata":{"execution":{"iopub.status.busy":"2024-12-01T17:11:20.851136Z","iopub.execute_input":"2024-12-01T17:11:20.851805Z","iopub.status.idle":"2024-12-01T17:12:34.355960Z","shell.execute_reply.started":"2024-12-01T17:11:20.851758Z","shell.execute_reply":"2024-12-01T17:12:34.354841Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LoRA Fine-tuning","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank (4, 8 or 16).\ngemma.backbone.enable_lora(rank=lora_rank)\ngemma.summary()\n\n# Limit the input sequence length (to control memory usage).\ngemma.preprocessor.sequence_length = token_limit\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=lr_value,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save LoRA for each epoch","metadata":{}},{"cell_type":"code","source":"class CustomCallback(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    model_name = f\"/kaggle/working/{lora_name}_{lora_rank}_epoch{epoch+1}.lora.h5\"\n    gemma.backbone.save_lora_weights(model_name)\n\n    # Evaluate\n    # text_gen(\"ᄃᆡ작ᄒᆞ여그ᄭᅩᆺ치흣터지거ᄂᆞᆯ\")\n    # text_gen(\n    #   \"금두겁이품의드러뵈니일졍ᄌᆡᄌᆞᄅᆞᆯ나흐리로다ᄒᆞ더니과연그ᄃᆞᆯ부터잉ᄐᆡᄒᆞ여십삭이차니\"\n    # )\n    # text_gen(\n    #   \"이ᄯᆡᄂᆞᆫᄉᆞ월초팔일이라이날밤의오ᄉᆡᆨ구룸이집을두루고향ᄂᆡ진동ᄒᆞ며션녀ᄒᆞᆫᄡᅣᆼ이촉을들고드러와김ᄉᆡᆼᄃᆞ려니르ᄃᆡ\"\n    # )\n    # text_gen(\"ᄌᆡ히길너텬졍을어긔지말으소셔이아희ᄇᆡ필은낙양니샹셔집아ᄌᆡ니\")\n    \n    text_gen(\"रामले भात खायो। \")\n    text_gen(\"कर्तब्य के छ जन को येति छन् बिरोध ? नक्श्यत्र हेर नभ मा दिल साथ सोध \")\n    text_gen(\"विश्व का सर्वोच्च शिखर सगरमाथा हॊ। \")\n\nhistory = gemma.fit(train, epochs=train_epoch, batch_size=1, callbacks=[CustomCallback()])\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Expansion Idea\n\nTo achieve similar tasks, you can replicate the same structure. Below are some examples:\n\n* American English <-> British English datasets\n\nVarious everyday objects and concepts have different names depending on the region. For example, in American English (AmE), people use terms like \"elevator,\" \"truck,\" \"cookie,\" and \"french fries,\" while in British English (BrE), the equivalent words are \"lift,\" \"lorry,\" \"biscuit,\" and \"chips,\" respectively.\n\nApart from vocabulary differences, spelling variations also exist. For instance, in AmE, words ending in \"-or\" are often spelled with \"-our\" in BrE. Examples include \"color\" (AmE) and \"colour\" (BrE), or \"humor\" (AmE) and \"humour\" (BrE).\n\nAnother spelling variation is the \"-ize\" versus \"-ise\" distinction. In AmE, words like \"organize\" and \"realize\" are commonly spelled with a \"z,\" whereas in BrE, the preferred spelling is \"organise\" and \"realise,\" using an \"s\" instead.\n\nWith the help of AI tools like Gemma, it is possible to create a style transfer from one English dialect to another, allowing seamless transitions between American and British English writing styles.\n\n* Kansai-ben datasets\n\nIn the Kansai region of Japan, there is a distinct group of dialects known as Kansai-ben. Compared to the standard Japanese language, Japanese speakers perceive Kansai-ben as being both more melodic and harsher in its pronunciation and intonation.\n\nUtilizing the capabilities of Gemma, you can create a dialect translator by preparing a substantial quantity of Kansai-ben datasets.","metadata":{}}]}